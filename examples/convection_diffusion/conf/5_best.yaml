hydra:
  run:
    # dynamic output directory according to running time and override name
    dir: outputs_case5/${now:%Y-%m-%d}/${now:%H-%M-%S}/${hydra.job.override_dirname}
  job:
    name: ${mode} # name of logfile
    chdir: false # keep current working direcotry unchaned
    config:
      override_dirname:
        exclude_keys:
          - TRAIN.checkpoint_path
          - TRAIN.pretrained_model_path
          - EVAL.pretrained_model_path
          - mode
          - output_dir
          - log_freq
  sweep:
    # output directory for multirun
    dir: ${hydra.run.dir}
    subdir: ./

log_grad_norm: false
log_loss: false

mode: train
seed: 42
output_dir: ${hydra:run.dir}
log_freq: 20
LAMBDA: 0.0625
EPS: 10.0
DA: 10.0
K0: 1.0
PE: 30.0
l: 2
L: 10
H: 1.0
T: 5
DT: 0.1
NY: 25

MT: 2
FDM_C_PATH: ./datasets/case5_fdm_reaction_txyc.csv
FDM_B_PATH: ./datasets/case5_fdm_reaction_txby.csv
MODEL:
  model_c:
    input_keys:
    - t
    - x
    - 'y'
    output_keys:
    - c10
    num_layers: 4
    hidden_size: 64
    output_dim: 10
    activation: tanh
  model_b:
    input_keys:
    - t
    - x
    - 'y'
    output_keys:
    - b
    num_layers: 1
    hidden_size: 16
    output_dim: 1
    activation: tanh
TRAIN:
  epochs: 3000
  l_bfgs_epochs: 150
  iters_per_epoch: 1
  eval_during_train: true
  eval_freq: 500
  lr_scheduler:
    epochs: ${TRAIN.epochs}
    iters_per_epoch: ${TRAIN.iters_per_epoch}
    learning_rate: 0.001
    gamma: 0.1
    by_epoch: true
  weight:
    IC: 100
    BC: 2000
    EQ: 30
    AD: 0.03
  pretrained_model_path: null
EVAL:
  pretrained_model_path: null
  eval_with_no_grad: true
  batch_size:
    metric_c: 8192
    metric_b: 8192
